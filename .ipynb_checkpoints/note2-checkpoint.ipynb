{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模（超参数选择）\n",
    "\n",
    "## 数据集划分\n",
    "- 将数据划分为训练集、验证集（validation set/development set/dev set）和测试集——训练集用于建模，验证集用于选取超参数，测试集用于测试模型效果。  \n",
    "- 以往经验划分（小规模）数据集：\n",
    "    - 70%训练集、30%测试集\n",
    "    - 60%训练集、20%验证集、20%测试集  \n",
    "- 但现代大数据量的数据集划分：\n",
    "    - 验证集和训练集只需要留少量数据验证模型参数和测试精度，大部分数据保留在训练集用于建模\n",
    "    - 比如1,000,000个数据，只需要10,000作为验证集，10,000作为测试集即可    \n",
    "    \n",
    "## 数据集来源\n",
    "- 现代数据的训练集和测试集常常是不同来源不同分布，比如训练集下载于网络，测试集来自于真实用户\n",
    "- 这种情况要保证验证集和测试集来自同一个来源，具有同样的分布\n",
    "- 或者只用训练集和验证集，不要测试集  \n",
    "\n",
    "## 机器学习基本策略\n",
    "- 高偏置：关注训练集效果，可以尝试：\n",
    "    - 更大的网络\n",
    "    - 更长的训练时间  \n",
    "    \n",
    "- 高次项：关注验证集效果，可以尝试：\n",
    "    - 更多数据\n",
    "    - 正则化  \n",
    "    \n",
    "# 正则化\n",
    "\n",
    "## logistic回归正则项\n",
    "常用L2范数  \n",
    "\n",
    "## 神经网络正则项\n",
    "- [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm),有时也叫做weight decay\n",
    "- 如何正则：比如用`tanh`作为激活函数的神经网络，如果正则项中$\\lambda$很大，相应的初始化在0附近的权重W会变小，进而$z^{[l]}=W^{[l]}a^{[l]}+b^{[l]}$进入`tanh`的线性部分（0附近），从而把网络变成线性回归，网络过拟合的可能性变小  \n",
    "- 何处加正则项：\n",
    "    - 损失函数\n",
    "    - 反向传播的权重矩阵更新项（梯度）\n",
    "\n",
    "## Dropout\n",
    "- 用于训练集，测试集不用\n",
    "- 常用语CV领域，因为常常没有足够多的数据，会过拟合，而每个数据（图片），又有过多的特征（像素）  \n",
    "\n",
    "## Data Augmentation\n",
    "图像作为数据时，可以用水平变换，旋转等手段生成‘新的’图片作为新数据，更多数据以防止过拟合的方法，类似于一种正则化\n",
    "## Early Stopping\n",
    "\n",
    "# 优化\n",
    "\n",
    "## Normalization\n",
    "以2维为例，未做normalization的数据的损失函数可能是椭圆，normalization后变为圆，做梯度下降时收敛更快  \n",
    "\n",
    "## 权重初始化\n",
    "- `Wl = np.random.randn(shape) * np.sqrt(2/n_prev)`适用于激活函数为RELU\n",
    "- Xavier initialization适用于激活函数为`tanh`\n",
    "- Yoshua Bengio提出的初始化$\\sqrt{\\frac{2}{n^{[l]}+n^{[l-1]}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization algorithms\n",
    "\n",
    "## Mini batch SGD\n",
    "- 在mini batch SGD中，对所有mini batch，即整个训练集进行一次遍历，叫做一个epoch，其中的步数（iteration），即为mini batch的个数\n",
    "- mini batch大小（两个极端）对收敛速度的影响：\n",
    "    - m=m(整个样本大小），mini-batch变为BGD，基本朝全局最小值收敛，但每步循环变慢\n",
    "    - m=1，mini-batch变为SGD，每步迭代速度快，但是噪声多，并失去向量化的意义  \n",
    "- 尺寸选择：\n",
    "    - 数据集小，直接用BGD\n",
    "    - 数据集大，可以在64,128,256,512中选择试验\n",
    "    - 适合CPU/GPU的内存大小  \n",
    "    \n",
    "## Momentum\n",
    "\n",
    "## RMSprop（Root Mean Square prop）\n",
    "$$S_dW = \\beta S_dW + (1-\\beta)dW^2$$\n",
    "$$S_db = \\beta S_db + (1-\\beta)db^2$$\n",
    "$$W:=W-\\alpha\\frac{dW}{\\sqrt{S_dW}}$$\n",
    "$$b:=b-\\alpha\\frac{db}{\\sqrt{S_db}}$$  \n",
    "\n",
    "减少梯度下降时因各个维度尺度不同而产生的震荡，作用类似于Momentum  \n",
    "\n",
    "## Adam（Adaptive Moment estimation）\n",
    "初始化$V_dW=0,S_dW=0.V_db=0,S_db=0$  \n",
    "\n",
    "第t次循环：\n",
    "$$V_dW=\\beta_1 V_dW+(1-\\beta_1)dW,V_db=\\beta_1 V_db+(1-\\beta_1)db\\tag{Momentum$\\beta_1$}$$\n",
    "$$S_dW=\\beta_2 S_dW+(1-\\beta_2)dW_2,S_db=\\beta_1 S_db+(1-\\beta_2)db\\tag{RMSprop$\\beta_2$}$$\n",
    "$$V^{corrected}_{dW}=\\frac{V_{dW}}{1-\\beta^t_1},V^{corrected}_{db}=\\frac{V_{db}}{1-\\beta^t_1}$$\n",
    "$$S^{corrected}_{dW}=\\frac{S_{dW}}{1-\\beta^t_2},S^{corrected}_{db}=\\frac{S_{db}}{1-\\beta^t_2}$$\n",
    "$$W:=W-\\alpha\\frac{V^{corrected}_{dW}}{\\sqrt{S^{corrected}_{dW}}+\\epsilon}$$\n",
    "$$b:=b-\\alpha\\frac{V^{corrected}_{db}}{\\sqrt{S^{corrected}_{db}}+\\epsilon}$$  \n",
    "\n",
    "- 超参数：\n",
    "    - $\\alpha$：需调整\n",
    "    - $\\beta_1$：一般取0.9\n",
    "    - $\\beta_2$：一般取0.99\n",
    "    - $\\epsilon$：一般取10e-8  \n",
    "    \n",
    "## learning rate decay\n",
    "- 常用decay方法：\n",
    "$$\\alpha=\\frac{1}{1+decay-rate*epoch-num}\\alpha_0$$  \n",
    "\n",
    "举例说明：比如初始学习率$\\alpha_0=0.2$,`decay-rate=1`，则每个epoch学习率计算如下——  \n",
    "\n",
    "|Epoch    |$\\alpha$    |\n",
    "|----------|-------------|\n",
    "|1      |0.1       |\n",
    "|2      |0.67       |\n",
    "|3      |0.5        |\n",
    "|4      |0.4        |  \n",
    "\n",
    "- 另外几种decay：\n",
    "    - $\\alpha=0.95^{epoch-num}\\cdot\\alpha_0$指数decay\n",
    "    - $\\alpha=\\frac{k}{\\sqrt{epoch-num}}\\cdot\\alpha_0$\n",
    "    - $\\alpha=\\frac{k}{\\sqrt{t}}\\cdot\\alpha_0$\n",
    "    - 经过数个iteration，把学习率减小一半  \n",
    "    \n",
    "## 局部最小值\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
